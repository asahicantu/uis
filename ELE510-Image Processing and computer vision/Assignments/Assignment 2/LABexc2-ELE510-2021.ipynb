{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# ELE510 Image Processing with robot vision: LAB, Exercise 2, Image Formation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Purpose:** *To learn about the image formation process, i.e. how images are projected from the scene to the image plane.*\n",
    "\n",
    "The theory for this exercise can be found in chapter 2 and 3 of the text book [1]. Supplementary information can found in chapter 1, 2 and 3 in the compendium [2]. See also the following documentations for help:\n",
    "- [OpenCV](https://opencv.org/opencv-python-free-course/)\n",
    "- [numpy](https://numpy.org/doc/stable/)\n",
    "- [matplotlib](https://matplotlib.org/stable/contents.html)\n",
    "\n",
    "**IMPORTANT:** Read the text carefully before starting the work. In\n",
    "many cases it is necessary to do some preparations before you start the work\n",
    "on the computer. Read necessary theory and answer the theoretical part\n",
    "frst. The theoretical and experimental part should be solved individually.\n",
    "The notebook must be approved by the lecturer or his assistant.\n",
    "\n",
    "**Approval:**\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The current notebook should be submitted on CANVAS as a single pdf file. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    To export the notebook in a pdf format, goes to File -> Download as -> PDF via LaTeX (.pdf).\n",
    "</div>\n",
    "\n",
    "**Note regarding the notebook**: The theoretical questions can be answered directly on the notebook using a *Markdown* cell and LaTex commands (if relevant). In alternative, you can attach a scan (or an image) of the answer directly in the cell.\n",
    "\n",
    "Possible ways to insert an image in the markdown cell:\n",
    "\n",
    "`![image name](\"image_path\")`\n",
    "\n",
    "`<img src=\"image_path\" alt=\"Alt text\" title=\"Title text\" />`\n",
    "\n",
    "\n",
    "**Under you will find parts of the solution that is already programmed.**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p>You have to fill out code everywhere it is indicated with `...`</p>\n",
    "    <p>The code section under `######## a)` is answering subproblem a) etc.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**a)** What is the meaning of the abbreviation PSF? What does the PSF specify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pont Spread Function (PSF)\n",
    "It is a function that defines how an object would look like when processed or visualized by an imaging system. It is also called impulse response. \n",
    "The PSD is a result of diffraction and interference.  The degree of spreading (blurring) of the object is a measure for the quality of an imaging system.\n",
    "\n",
    "## Alternaive answer\n",
    "The point spread function (PSF) describes the response of an imaging\n",
    "system to a point source. This is more generally known as a systems impulse response. The Point\n",
    "spread function (PSF) specifies the shape that a point will take on the image pane.\n",
    "\n",
    "The term “impulse response” is common in signal processing, and if the system is a filter (rather\n",
    "than the imaging system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**b)** Use the imaging model shown in Figure 1. The camera has a lens with focal length $f = 40\\text{mm}$ and in the image plane a CCD sensor of size $8\\text{mm} \\times 8\\text{mm}$. The total number of pixels is $4000 \\times 4000$. How many lines per mm will this camera resolve at a distance of $z_w = 1\\text{m}$ from the camera center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<img src=\"./images/perspectiveProjection.jpg\" alt=\"Alt text\" title=\"Title text\" />\n",
    "\n",
    "**Figure 1**: Perspective projection caused by a pinhole camera. Figure 2.23 in [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Point CCD Sensor (Charge-coupled device   ) \n",
    "Assumed to have an array of photodiodes spaced \n",
    "$$d=8mm/4000pixels=2 \\mu m/pixel(line)$$\n",
    "\n",
    "$$\\frac{D}{1m}=\\frac{d}{f}=\\frac{d}{40mm}$$\n",
    "\n",
    "$$D=\\frac{2 \\cdot 10^{-6}}{40 \\cdot 10^{-6}} \\cdot 1m = \\frac{1}{20}mm/pixel(line)$$\n",
    "\n",
    "**20 lines per mm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**c)** Explain how a Bayer filter works. What is the alternative to using this type of filter in image acquisition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayer filter\n",
    "Is an array of color filters arranged in a square grid of photosensors. The filter pattern is half green, one quarter red and one quarter blue, hence is also called BGGR or any combination or double Gren (BB). It mimics the physiology of the human eye whose combination of type M and L cone cells during daylight vision are more sensitive to green light.\n",
    "\n",
    "Information of the color combination is calculatedd by debayering algorithms which interpolate the missing information from neighboring pixels and estimate it. This involves image processing that is implemented on the same chip as the image\n",
    "sensor.This process, though can lead to false representation of image quality.\n",
    "\n",
    "An alternative to this architecture is found in the CMY color combination (Cyan, Magenta, Yellow) a set of inverted colors that have and improved light absorption characteristic.\n",
    "Other filter color patterns can be found for different camera manufacturers such as\n",
    "* Fujifilm \"EXR\" color filter array\n",
    "* Fujifilm \"X-trans\" filter\n",
    "* Quad Bayer\n",
    "\n",
    "## Propposed solution\n",
    "A Bayer filter blocks all but the green light for alternating pixels throughout the sensor in a checkerboard pattern. Green is then sensed by half the pixels, with the remaining\n",
    "pixels sensing blue or red in alternating rows. The alternative to using this type of filter is to use\n",
    "three sensors to capture the image, one for each color channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 2\n",
    "\n",
    "Assume we have captured an image with a digital camera. The image covers an area in the scene of size $1.024\\text{m} \\times 0.768\\text{m}$ (The camera has been pointed towards a wall such that the distance is approximately constant over the whole image plane, *weak perspective*). The camera has 2048 pixels horizontally, and 1536 pixels vertically. The active region on the CCD-chip is $10\\text{mm} \\times 7.5\\text{mm}$. We define the spatial coordinates $(x_w,y_w)$ such that the origin is at the center of the optical axis, x-axis horizontally and y-axis vertically upwards. The image indexes $(x,y)$ is starting in the upper left corner. For simplicity let the optical axis meet the image plane at $(x_{0}=1024,y_{0}=768)$. The solutions to this problem can be found from simple geometric considerations. Make a sketch of the situation and answer the following questions:\n",
    "\n",
    "**a)** What is the size of each sensor (one pixel) on the CCD-chip?\n",
    "\n",
    "**b)** What is the scaling coefficient between the image plane (CCD-chip) and the scene? What is the scaling coefficient between the scene coordinates and the image indexes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sketch\n",
    "![Sketch Image](./images/sketch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) What is the size of each sensor (one pixel) on the CCD-Chip?\n",
    "$$Scene_{Size}=1024mm \\times 768mm$$\n",
    "$$Camera_{resolution}=2048 \\times 1536 \\space pixels$$\n",
    "$$CCD_{ActiveRegion}=10mm \\times 7.5mm$$\n",
    "$$Pixel_{sizeX}=\\frac{10mm}{2048pixels}=0.0048mm=4.8 \\mu m$$\n",
    "$$Pixel_{sizeY}=\\frac{7.5mm}{1536pixels}=0.0048mm=4.8 \\mu m$$\n",
    "$$Pixel_{Size}=4.8 \\mu m \\times 4.8 \\mu m $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T13:46:10.469876Z",
     "start_time": "2021-09-11T13:46:10.445874Z"
    }
   },
   "source": [
    "### b) What is the scaling coefficient between the image plane (CCD-Chip) and the scene?\n",
    "$$C_x=\\frac{10mm}{1024mm}=0.097$$  \n",
    "$$C_y=\\frac{7.5mm}{768mm}=0.097$$\n",
    "**The coefficient is  0.97 to 1 or 97 times smaller in the CCD-Chip than in the original scene**\n",
    "\n",
    "### What is the scaling coefficient between the scene coordinates and the image pixels?\n",
    "$$C_x=\\frac{1024}{2048}=0.5$$  \n",
    "$$C_y=\\frac{768}{1536}=0.5$$\n",
    "**The coefficient is 1 to 1/2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 3\n",
    "\n",
    "Translation from the scene to a camera sensor can be done using a transformation matrix, $T$. \n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{bmatrix} x\\\\y\\\\1\\end{bmatrix} = \n",
    "\tT\n",
    "\t\\begin{bmatrix}\n",
    "\t\tx_w\\\\ y_w\\\\ 1\n",
    "\t\\end{bmatrix}\\\\\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\tT= \\begin{bmatrix} \\alpha_x & 0 & x_0\\\\\n",
    "\t\t\t0 & \\alpha_y & y_0\\\\\n",
    "\t\t0   & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$\\alpha_x$ and $\\alpha_y$ are the scaling factors for their corresponding axes.\n",
    "\n",
    "Write a function in Python that computes the image points using the transformation matrix, using the parameters from Problem 2. Let the input to the function be a set of $K$ scene points, given by a $2 \\times K$ matrix, and the output the resulting image points also given by a $2 \\times K$ matrix. The parameters defining the image sensor and field of view from the camera center to the wall can also be given as input parameters.\n",
    "\n",
    "Test the function for the following input points given as a matrix:\n",
    "\\begin{equation}\\label{cam-eq4}\n",
    "    {\\mathbf P}_{in} = \\begin{bmatrix} 0.512 & -0.512 & -0.512 & 0.512 & 0 & 0.3 & 0.3 & 0.3 & 0.6 \\\\\n",
    "    0.384 & 0.384 & -0.384 & -0.384 & 0 & 0.2 & -0.2 & -0.4 & 0 \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Comment on the results, especially notice the two last points!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T14:01:46.378047Z",
     "start_time": "2021-09-11T14:01:45.662535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the packages that are useful inside the definition of the weakPerspective function\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:47:26.165499Z",
     "start_time": "2021-09-11T15:47:26.147467Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that takes in input:\n",
    "- FOV: field of view,\n",
    "- sensorsize: size of the sensor,\n",
    "- n_pixels: camera pixels,\n",
    "- p_scene: K input points (2xK matrix)\n",
    "\n",
    "and return the resulting image points given the 2xK matrix\n",
    "\"\"\"\n",
    "def weakPerspective(FOV, sensorsize, n_pixels, p_scene):\n",
    "    scale_coefficient_scene = sensorsize/FOV\n",
    "    scale_coefficient_sensor = FOV/n_pixels\n",
    "    \n",
    "    scale= scale_coefficient_scene * scale_coefficient_sensor\n",
    "    p_scene = np.vstack([p_scene, np.ones(p_scene.shape[1])])\n",
    "\n",
    "    T = np.array([[scale[0],0,FOV[0]], [0,scale[1],FOV[1]], [0,0,1]])\n",
    "    \n",
    "    result = np.round( T @ p_scene,2) \n",
    "    result.dtype = 'float32'\n",
    "    return  result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:47:26.512104Z",
     "start_time": "2021-09-11T15:47:26.500104Z"
    }
   },
   "outputs": [],
   "source": [
    "# The above function is then called using the following parameters:\n",
    "\n",
    "# Parameters\n",
    "FOV = np.array([1024, 768])  #plane size giveninmilimiters\n",
    "sensorsize = np.array([10, 7.5]) # CCD Sensor size\n",
    "n_pixels = np.array([2048, 1536])\n",
    "p_scene_x = [0.512, -0.512, -0.512, 0.512, 0, 0.3, 0.3, 0.3, 0.6]\n",
    "p_scene_y = [0.384, 0.384, -0.384, -0.384, 0, 0.2, -0.2, -0.4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:47:26.918638Z",
     "start_time": "2021-09-11T15:47:26.902591Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    4.5   0.    4.5   0.    4.5   0.    4.5   0.    4.5   0.    4.5\n",
      "  0.    4.5   0.    4.5   0.    4.5  ]\n",
      " [0.    4.25  0.    4.25  0.    4.25  0.    4.25  0.    4.25  0.    4.25\n",
      "  0.    4.25  0.    4.25  0.    4.25 ]\n",
      " [0.    1.875 0.    1.875 0.    1.875 0.    1.875 0.    1.875 0.    1.875\n",
      "  0.    1.875 0.    1.875 0.    1.875]]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This cell is locked; it can be only be executed to see the results. \n",
    "####\n",
    "# Input data:\n",
    "p_scene = np.array([p_scene_x, p_scene_y])\n",
    "\n",
    "# Call to the weakPerspective() function \n",
    "pimage = weakPerspective(FOV, sensorsize, n_pixels, p_scene)\n",
    "\n",
    "# Result: \n",
    "print(pimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "\n",
    "### Delivery (dead line) on CANVAS: 12-09-2021 at 23:59\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Contact\n",
    "### Course teacher\n",
    "Professor Kjersti Engan, room E-431\n",
    "\n",
    "E-mail: kjersti.engan@uis.no\n",
    "\n",
    "### Teaching assistant\n",
    "Tomasetti Luca, room E-401\n",
    "\n",
    "E-mail: luca.tomasetti@uis.no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## References\n",
    "\n",
    "[1] S. Birchfeld, Image Processing and Analysis. Cengage Learning, 2016.\n",
    "\n",
    "[2] I. Austvoll, \"Machine/robot vision part I,\" University of Stavanger, 2018. Compendium, CANVAS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
